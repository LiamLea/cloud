task_id: 8

ntp:
  server:
  - ntp.aliyun.com

timezone: Asia/Shanghai

dns:
- 114.114.114.114
- 8.8.8.8

hosts:
- ip: 127.0.0.1
  hostname:
  - localhost

http_proxy:
  server: http://10.10.10.250:8123
  no_proxy: "10.0.0.0/8"  #service and pod cidr

#if registry is changed,you should pay attention to registry_prefix
registry: "10.10.10.250"
#when registry is empty, this should be empty too
#must end with / or be empty
registry_prefix: "{{ registry }}/library/"

docker:
  version: 20.10.5
  containerd:
    sandbox_image: "{{ registry_prefix }}k8s.gcr.io/pause:3.6"
  insecure-registries: ["{{ registry }}"]
  registry-mirrors: []
  http_proxy:
    enabled: True
    server: "{{ http_proxy.server }}"
    #must include service and pod network: 10.0.0.0/8
    no_proxy: ["{{ registry }}", "10.0.0.0/8", "quay.io", "docker.com", "docker.io", "localhost", "127.0.0.1", "aliyuncs.com", "myhuaweicloud.com"]

kubernetes:
  version: 1.22.10
  repository: "{{ registry_prefix }}k8s.gcr.io"
  apiserver:
    control_plane:
      ip: 10.172.1.10
      port: 16443
    network:
      service_subnet: 10.96.0.0/12
      pod_subnet: 10.244.0.0/16
    nodeport_range: 10000-65535
  backup:
    image: "{{ registry_prefix }}bitnami/etcd:3.5.6"
    schedule: "0 0 * * *"
    dir: /tmp/
    #how many days will these files keep
    reservation: 7

calico:
  repository: "{{ registry_prefix }}"
  encapsulation: IPIP
  nodeAddressAutodetectionV4:
    cidrs:
    - "10.172.1.0/24"

nginx:
  image: "{{ registry_prefix }}nginx:latest"
  health_check:
    port: 16442

keepalived:
  id: 120
  vip: "{{ kubernetes.apiserver.control_plane.ip }}"
  health_check:
    url: "http://127.0.0.1:{{ nginx.health_check.port }}/"
  image: "{{ registry_prefix }}osixia/keepalived:latest"

chart:
  http_proxy:
    enabled: True
    server: "{{ http_proxy.server }}"
    no_proxy: "{{ kubernetes.apiserver.control_plane.ip }}, {{ http_proxy.no_proxy }}"
  repo:
    ceph-csi: https://ceph.github.io/csi-charts
    prometheus-community: https://prometheus-community.github.io/helm-charts
    grafana: https://grafana.github.io/helm-charts
    elastic: https://helm.elastic.co
    #bitnami: https://charts.bitnami.com/bitnami
    bitnami: https://raw.githubusercontent.com/bitnami/charts/archive-full-index/bitnami
    cert-manager: https://charts.jetstack.io
    ingress-nginx: https://kubernetes.github.io/ingress-nginx
    paradeum-team: https://paradeum-team.github.io/helm-charts/
    kfirfer: https://kfirfer.github.io/helm/
    kafka-ui: https://provectus.github.io/kafka-ui

  #if the charts are local, set local directory
  #or will use the remote repo
  #local_dir: "/root/ansible/charts" ,this is path in container, so you should put charts in the current directory/charts
  local_dir: ""

storage_class:
  repository: "{{ registry_prefix }}"
  ceph:
    enabled: true
    cluster:
      #id and mons(get from /etc/ceph/ceph.conf)
      id: ""
      mons: []
      #id and key of and user(get from /etc/ceph/*.ketring, e.g. id: admin,  key: 1111)
      admin:
        id: ""
        key: ""
    cephfs:
      namespace: "ceph-csi-cephfs"
      name: "ceph-csi-cephfs"
      chart:
        repo: "{{ chart.repo['ceph-csi'] }}"
        path: "ceph-csi-cephfs"
        version: "3.4.0"
      config:
        default: false
        class_name: "csi-cephfs-sc"
        #volume must exist
        fs_name: "ceph-fs"
        volume_name_prefix: "dev-k8s-vol-"
        #host network mode, so this port must be available
        nodeplugin_metric_port: 8082

    rbd:
      namespace: "ceph-csi-rbd"
      name: "ceph-csi-rbd"
      chart:
        repo: "{{ chart.repo['ceph-csi'] }}"
        path: "ceph-csi-rbd"
        version: "3.4.0"
      config:
        default: true
        class_name: "csi-rbd-sc"
        pool: "rbd-replicated-pool"
        volume_name_prefix: "dev-k8s-vol-"
        #host network mode, so this port must be available
        nodeplugin_metric_port: 8081


basic:
  namespace: basic-public
  repository: "{{ registry_prefix }}"
  #use default sc when set null
  storage_class: null
  cert_manager:
    name: "cert-manager"
    chart:
      repo: "{{ chart.repo['cert-manager'] }}"
      path: "cert-manager"
      version: "1.7.2"
  ingress_nginx:
    name: "ingress-nginx"
    chart:
      repo: "{{ chart.repo['ingress-nginx'] }}"
      path: "ingress-nginx"
      version: "4.0.18"
    config:
      ingress_class: nginx
      http_port: 80
      https_port: 443

#speficy domain
domains:
- "home.liamlea.local"
- "home.bongli.local"
ingress:
  enabled: true
  cluster_issuer: ca-issuer
  class_name: "{{ basic.ingress_nginx.config.ingress_class }}"
  hosts:
    prometheus:
      host: ""
      domains: "{{ domains }}"
      path: "/prometheus"
    alertmanager:
      host: ""
      domains: "{{ domains }}"
      path: "/alertmanager"
    grafana:
      host: ""
      domains: "{{ domains }}"
      path: "/grafana"
    kibana:
      host: ""
      domains: "{{ domains }}"
      path: "/kibana"
    adminer:
      host: "adminer"
      domains: "{{ domains }}"
      path: "/"
    redis-commander:
      host: "redis-commander"
      domains: "{{ domains }}"
      path: "/"
    kafka-ui:
      host: "kafka-ui"
      domains: "{{ domains }}"
      path: "/"

monitor:
  namespace: monitor
  repository: "{{ registry_prefix }}"
  #use default sc when set null
  storage_class: null
  prometheus:
    name: prometheus
    chart:
      repo: "{{ chart.repo['prometheus-community'] }}"
      #the path is relative path
      path: "prometheus"
      version: "15.8.5"
    resources:
      requests:
        cpu: "4000m"
        memory: "8Gi"
      limits:
        cpu: "4000m"
        memory: "8Gi"
      storage: 16Gi
    config:
      #this url will send with alerting msg which user can click to access prometheus to get alerting details
      #  e.g. https://k8s.my.local:30443/prometheus
      #set /prometheus if you don't know the exact external_url or you can't access prometheus
      external_url: "https://{{ domains[0] }}:{{ basic.ingress_nginx.config.https_port }}/prometheus"
      # e.g. {send_resolved: true, url: "<url|no empty>", max_alerts: 10}
      webhook_configs: []
      # e.g. {send_resolved: true, smarthost: smtp.qq.com:587 ,from: 1059202624@qq.com, to: 1059202624@qq.com, auth_username: 1059202624@qq.com, auth_password: 'xx'}
      email_configs: []
      scrape_interval: "30s"
      #e.g. "10.10.10.1"
      icmp_probe: []
      #e.g. "10.10.10.1:80"
      tcp_probe: []
      #e.g. "http://10.10.10.1:80/test"
      http_probe: []
      #e.g. "10.10.10.1:9092"
      node_exporter: []
      jobs:
        #e.g. {"targets": ["10.10.10.10:9283"], "labels": {cluster: "openstack"}}
        ceph_exporter: []
    blackbox:
      name: "blackbox-exporter"
      chart:
        repo: "{{ chart.repo['prometheus-community'] }}"
        path: "prometheus-blackbox-exporter"
        version: "5.7.0"
    adapter:
      name: "prometheus-adapter"
      chart:
        repo: "{{ chart.repo['prometheus-community'] }}"
        path: "prometheus-adapter"
        version: "3.2.2"
    grafana:
      name: "grafana"
      chart:
        repo: "{{ chart.repo['grafana'] }}"
        path: "grafana"
        version: "6.28.0"
      dashboards:
        default:
          node-exporter:
            gnetId: 8919
            revision: 24
            datasource: Prometheus
          node-full:
            gnetId: 1860
            revision: 27
            datasource: Prometheus
          blackbox-exporter:
            gnetId: 7587
            revision: 3
            datasource: Prometheus
          #you may need to add a variable represents datasource or this dashboard would go wrong
          k8s:
            gnetId: 15520
            revision: 1
            datasource: Prometheus
          k8s-pv:
            gnetId: 11454
            revision: 14
            datasource: Prometheus
          apiserver:
            gnetId: 12006
            revision: 1
            datasource: Prometheus
          etcd:
            gnetId: 9733
            revision: 1
            datasource: Prometheus
          ceph:
            gnetId: 2842
            revision: 14
            datasource: Prometheus

  node_exporter:
    version: 1.3.0
    #will download node-exporter binary when local_dir is empty
    # this is binary file
    local_dir: "/root/ansible/files/node_exporter"
    install_path: /usr/local/bin/
    port: 9100

  exporters:
    openstack_exporter:
      target_config:
        clouds: {}
          # cloud-1:
          #   region_name: RegionOne
          #   auth:
          #     username: admin
          #     password: cangoal
          #     project_name: admin
          #     project_domain_name: 'Default'
          #     user_domain_name: 'Default'
          #     auth_url: 'http://10.10.10.10:35357/v3'
          #     verify: false

log:
  namespace: log
  repository: "{{ registry_prefix }}"
  #use default sc when set null
  storage_class: null
  kafka:
    #if false, must set bootstrap_servers to use the existing kafka
    #if true, use service.kafka vars above to install new kafka
    enabled: true
    bootstrap_servers: []

  elastic:
    version: "7.17.3"
    security:
      password: "elastic"
    elasticsearch:
      name: elasticsearch
      chart:
        repo: "{{ chart.repo['elastic'] }}"
        #the path is relative path
        path: "elasticsearch"
      resources:
        requests:
          cpu: "8000m"
          memory: "16Gi"
        limits:
          cpu: "8000m"
          memory: "16Gi"
        storage: 30Gi
        #jvm heap best practice（half of the total memory and less than 30G）
        esJavaOpts: "-Xmx8g -Xms8g"
    kibana:
      name: kibana
      chart:
        repo: "{{ chart.repo['elastic'] }}"
        path: "kibana"
    logstash:
      name: logstash
      chart:
        repo: "{{ chart.repo['elastic'] }}"
        path: "logstash"
      replicas: "3"
      resources:
        requests:
          cpu: "100m"
          memory: "2Gi"
        limits:
          cpu: "1000m"
          memory: "2Gi"
        logstashJavaOpts: "-Xmx1g -Xms1g"
      config:
        batch_size: 1000
        group_id: logstash_log_k8s
    filebeat:
      name: filebeat
      chart:
        repo: "{{ chart.repo['elastic'] }}"
        path: "filebeat"

service:
  namespace: public
  repository: "{{ registry_prefix }}"
  #use default sc when set null
  storage_class: null
  kafka:
    enabled: true
    name: kafka
    chart:
      repo: "{{ chart.repo['bitnami'] }}"
      path: "kafka"
      version: "16.2.10"
    replicas: 3
    node_ports: [19092, 19093, 19094]
    #if empty work_master ip will be used
    domain: ""
    resources:
      heapOpts: "-Xmx1024m -Xms1024m"
      requests:
        cpu: "250m"
        memory: "1.5Gi"
      limits:
        cpu: "1000m"
        memory: "1.5Gi"
      zookeeper:
        jvmFlags: "-Xmx1024m -Xms1024m"
        requests:
          cpu: "250m"
          memory: "1.5Gi"
        limits:
          cpu: "1000m"
          memory: "1.5Gi"

  mysql:
    enabled: true
    name: mysql
    chart:
      repo: "{{ chart.repo['bitnami'] }}"
      path: "mysql"
      version: "8.5.7"
    root_password: cangoal
    replication_password: cangoal
    #standalone or replication
    architecture: replication
    resources:
      requests:
        cpu: "4000m"
        memory: "8Gi"
      limits:
        cpu: "4000m"
        memory: "8Gi"
      storage: 10Gi

  pgsql:
    enabled: true
    name: postgresql
    chart:
      repo: "{{ chart.repo['bitnami'] }}"
      path: "postgresql"
      version: "11.7.3"
    postgres_password: cangoal
    replication_password: cangoal
    #standalone or replication
    architecture: replication
    resources:
      requests:
        cpu: "4000m"
        memory: "8Gi"
      limits:
        cpu: "4000m"
        memory: "8Gi"
      storage: 10Gi

  redis:
    enabled: true
    name: redis
    chart:
      repo: "{{ chart.repo['bitnami'] }}"
      path: "redis"
      version: "17.0.8"
    password: cangoal
    resources:
      requests:
        cpu: "2000m"
        memory: "4Gi"
      limits:
        cpu: "2000m"
        memory: "4Gi"
      storage: 5Gi
    replica:
      count: 2

  tools:
    enabled: true

    #database tool
    adminer:
      name: adminer
      chart:
        repo: "{{ chart.repo['paradeum-team'] }}"
        path: "adminer"
        version: "0.1.9"

    #redis tool
    redis:
      name: redis-commander
      chart:
        repo: "{{ chart.repo['kfirfer'] }}"
        path: "redis-commander"
        version: "0.1.3"

    #kafka tool
    kafka:
      name: kafka-ui
      chart:
        repo: "{{ chart.repo['kafka-ui'] }}"
        path: "kafka-ui"
        version: "0.4.1"
      auth:
        user: admin
        password: cangoal
      #add kafka clusters
      kafka_clusters:
      - name: kafka-k8s
        bootstrapServers: kafka.public:9092

service_mesh:
  providers:
    prometheus:
      service: "prometheus-server.{{monitor.namespace}}.svc.cluster.local"
      port: 80
      path: "/prometheus"
    tracing:
      service: zipkin.istio-system.svc.cluster.local
      port: 9411
      path: ""
  telemetry:
    metrics: {}
    tracing:
      randomSamplingPercentage: 100
      disableSpanReporting: false
    accessLogging:
      disabled: true

download_packages:
  context: "centos-7"
  download_dir: "/tmp/download/"
  packages_list:
  - name: docker-ce
    version: 20.10.5
  - name: kubeadm
    version: 1.22.10
  - name: kubelet
    version: 1.22.10
  - name: kubectl
    version: 1.22.10
  contexts:
    centos-7:
      image: "centos:7"
      package_manager: "yum"
    ubuntu-18.04:
      image: "ubuntu:18.04"
      package_manager: "apt"
