###############################################################################
补充
  判断服务能否部署在同一台机器上:根据是否有资源冲突
  比如:DataNode 消耗硬盘资源
       NodeManager 消耗的是cpu和内存资源
       所以两者资源不冲突,能够最大化系统的资源,所以部署在同一台机器上

###############################################################################

一.部署mapreduce   //该框架都是开发使用的
#所有主机修改配置文件:mapred-site.xml
#先复制模板在修改
<configuration>
  <property>
    <name>mapreduce.framework.name</name>   //指定框架,只有local和yarn两种
    <value>yarn</value>
  </property>
</configuration>

二.部署yarn
1.所有主机配置yarn:yarn-site.xml
<configuration>
  <property>
    <name>yarn.resourcemanager.hostname</name>   //指定ResourceManager
    <value>nn01</value>
  </property>
  <property>
    <name>yarn.nodemanager.aux-services</name>    //指定使用哪个计算框架
    <value>名字</value>    //我们使用的框架是mapreduce_shuffle
  </property>
</configuration>

2.启动yarn
  sbin/start-yarn.sh

3.验证
  jps
  bin/yarn node -list

4.web访问hadoop
  访问NameNode页面:
    http;//ip:50070/
  访问secondary NameNode页面:
    http://ip:50090/
  访问DataNode页面
    http://ip:50075/
  访问ResourceManager页面
    http://ip:8088/
  访问NodeManager页面
    http://ip:8042/

###############################################################################
通过hadoop(即客户端)使用hdfs
  
bin/hadoop fs -命令 ...      //hadoop为hadoop集群的管理命令
  如:bin/hadoop fs -ls /    
#由于使用的是集群系统,所以实际的路径是:http://ip:9000/abc
     bin/hadoop fs -mkdir /abc   
     bin/hadoop fs -touchz /abc/a.txt
     bin/hadoop fs -put 本地磁盘路径 虚拟磁盘路径
     bin/hadoop fs -get 虚拟磁盘路径 本地磁盘路径

调用mapreduce计算程序,进行词频分析
  bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.7.jar wordcount 输入目录 输出目录
  bin/hadoop fs -cat 输出目录/*

###############################################################################
HDFS集群管理

一.添加dataNode
  1.启动一个主机,做好准备
  2.修改NameNode的slaves文件
  3.同步NameNode的/usr/local/hadoop到dataNode
  4.在新节点上执行启动命令
    sbin/hadoop-daemon.sh start datanode
  5.NameNode设置同步带宽
    bin/hdfs dfsadmin -setBalancerBandwidth 60000000  //50MB/s 千兆网卡的速率为123MB/s
  6.NamoNode同步数据
    sbin/start-balancer.sh
#bin/hdfs dfsamin -report 查看状态

二.修复节点
  只需要启动节点即可,数据会自动恢复
  sbin/hadoop-daemon.sh start datanode

三.删除节点(在NameNode执行)
  1.修改hdfs-site.xml,添加:
#按照之前的格式添加
    key:    dfs.hosts.exclude
    value:  /usr/local/hadoop/etc/hadoop/exclue 
  2.创建exclude文件,并添加内容:
    主机名        //要删除的节点的主机名
    ...
  3.执行数据迁移
    bin/hdfs dfsadmin -refreshNodes
  4.查看状态;
    Normal      //正常状态
    Decommissioned in process   //数据正在迁移
    Decommissioned     //数据迁移完成
#只有数据迁移完成,才能down机下线

###############################################################################
yarn节点管理

在需要管理的节点上执行:
  添加:
    sbin/yarn-daemon.sh start nodemanager
  删除:
    sbin/yarn-daemon.sh stop nodemanager
  查看:
    bin/yarn node -list

###############################################################################
nfsgw

通过nfs与hdfs客户端的交互,用户可以直接通过访问nfs访问hdfs集群

1.在NameNode和nfs主机上创建代理用户
#代理用户的uid,gid,用户名必须都相同
  groupadd -g 800 nfsuser
  useradd -u 800 -g 800 -r -d /var/hadoop nfsuser

2.NameNode修改配置文件:core-site.xml
  name:   hadoop.proxyuser.代理用户.groups
  value:  *
  name:   hadoop.proxyuser.代理用户.hosts
  value:  *

3.停止集群所有服务
  sbin/stop-all.sh

4.同步配置文件到所有主机

5.启动hdfs
  sbin/start-dfs.sh


nfs主机配置

1.环境配置
  卸载rpcbind,nfs-utils
  有各节点的主机名解析
  安装java环境
2.修改配置文件:hdfs-site.xml
  name:   nfs.exports.allowed.hosts
  value:  * rw
  name:   nfs.dump.dir    
  value:  /var/nfstmp
#转储目录,顺序的写操作会随机达到NFS网关,这个目录用于临时存储无序的写操作,然后传递给hdfs客户端

3.创建转储文件夹
  mkdir /var/nfstmp
  chown nfsuser.nfsuser /var/nfstmp

4.启动nfs服务
  setfacl -m user:nfsuser:rwx /usr/local/hadoop/logs
  sbin/hadoop-daemon.sh --script ./bin/hdfs start portmap
  sudo -u nfsuser ./sbin/hadoop-daemon.sh --script ./bin/hdfs start nfs3
#portmap 提供端口号注册的机制(之前是rpcbind提供的)
#先启动portmap


客户端挂载:
  mount -t nfs -o vers=3,proto=tcp,noatime,nolock,sync,noacl ip:/ /mnt
#vers=3     nfs只能使用v3版本
#proto=tcp  仅使用TCP作为传输协议
#noatime    禁止access time的时间更新,加快io性能
#nolock     在客户端加锁,同一个客户端的多个进程访问同一个文件不冲突,保证了数据的顺序存储
#lock       在服务器端枷锁,多个客户端的数据不会发生冲突
#sync       内存数据实时写入磁盘
#noacl      禁用acl扩展权限

###############################################################################
