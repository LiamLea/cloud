###############################################################################
zookeeper

保证事物的一致性,相当于交警,防止程序之间抢占资源(zookeeper是给程序使用的)
应用场景: 集群分布式锁
          集群统一命名
          分布式协调服务

角色:
  leader    //相当于master,与所有foller进行内部数据交换
  follwer   //参与投票
  observer  //相当于foller,只是不参与投票,因为投票多,会影响集群的性能

工作原理:
  leader是通过选取产生的,必须获得超过一半的票数(总票数不会因为有主机宕机而变少)
  所需至少需要3台主机才能实现可高用,当一台宕机后,可以选举出新的leader
  读操作,每个节点都可以单独相应
  写操作,需要发给leader,然后发起投票

###############################################################################
部署zookeeper

1.装包

2.所有主机修改配置文件:zoo.cfg
#添加,如果是observer在后面声明
#2888设置的是leader监听的端,3888是非leader监听的端口

  server.id号=主机名:2888:3888[:observer]   
  ...

3.根据配置文件中的datadir项创建数据目录:zookeeper

4.设置各主机的id号
  echo id号 > zookeeper/myid

5.启动各个节点
  zookeeper/bin/zkServer.sh start

6.验证
  nc 主机名 2181
  jps
  zookeeper/bin/zkServer.sh status   

7.编写查看集群状态的脚本:zastats.sh
#!/bin/bash

function getzkstat(){
  exec 2>/dev/null                        //将错误信息都扔掉
  exec 8<>/dev/tcp/$1/2182                //将后面的文件设置为8号的输入和输出
                                      //后面的文件表示一个socket,协议为tcp,主机名为$1,端口号为2182
  echo stat >& 8                         //通过socket发送stat指令,获取集群的状态
  message=`cat <& 8 | grep -P "^Mode:"`
  echo -e "$1\t${message:-Mode:NULL}"    //如果message没有值,则输出Mode:NULL
  exec 8<& -                             //将8号的输入关闭
}
for i in $@
do
  getzkstat $i
done

###############################################################################
kafka(给程序用)

分布式的消息系统,实现异步通信(相当于中介的身份),缓冲,解耦等功能
kafaka通过zookeeper管理集群,所以要先搭好zookeeper集群

一.角色:
  producer    //生产者,负责发布消息
  consumer    //消费者,负责读取处理消息
  topic       //消息类别(包含一个或多个partion)
  broker      //一台kafka服务器就是一个broker

二.部署kafka

1.所有主机修改配置文件:server.properties
  broker.id=数字          //每个主机不同
  zookeeprt.connect=主机1:2181,主机2:2181   //可以多写几台

2.所有主机启动kafkaf服务
  bin/kafka-server-start.sh -daemon config/server.properties

3.验证(需要用写好的程序进行验证)
  jps

(1)在某一个主机上创建一个topic
  kafka/bin/kafka-topics.sh --create --partions 2 --replication-factor 2 --zoookeeper localhost:2181 --topic mymsg
(2)生产者(负责发送消息)
  kafka/bin/kafka-console-producer.sh --broker-list localhost:9092 --topic mymsg
(3)消费者(负责接收和处理消息)
  kafka/bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic mymsg
   
###############################################################################
hadoop的高可用集群(实现NameNode和ResourceManager的高可用

第一种解决方案:HDFS with NFS
  两台namenode
  将NameNode的数据存储到nfs或者ceph上,实现nfs的高可用即可

第二种解决方案:HDFS with QJM
  不需要共享存储,但需要让每一个DataNode都知道两个NameNode的位置
  将多个NameNode加入到一个组内,datanode只需与该组交互即可
  从而把块信息和心跳包发送给Active和standby这两个NameNode
  为了让Standby Node和Active Node保持同步,这两个NameNode都与JNS(journal nodes)进程保持通信
  每当Active Node更新了映射空间,就会将变更日志发给JNS,然后standby Node进行同步

###############################################################################
部署hadoop高可用(利用zookeeper)

一.环境准备(添加一台NameNode)
  能够免密登录其他主机(且用同一对秘钥)
  java
  修改所有主机的/etc/hosts

二.修改配置文件
(1)hadoop-env.sh   //与之前一样
(2)slaves          //与之前一样
(3)mapred-site.xml  //与之前一样
(4)core-site.xml
  name:   fs.defaultFS          //指明namenode的位置
  value:  hdfs://组名           //指向一个组
  
  name:   ha.zookeeper.quorum   //指定zookeeper集群
  value:  主机1:2181,主机2:2181 //多写几台

(5)hdfs-site.xml    //设置namenode的高可用

#删除之前指定namenode和secondary namenode的配置
  name:   dfs.nameservices
  value:  组名

  name:   dfs.ha.namenodes.组名
  value:  nn1,nn2       //角色名,随便起

#有几个角色写几个键值对
  name:   dfs.namenode.rpc-address.组名.角色名  //指明角色对应的主机
  value:  主机名:8020

#有几个角色写几个键值对
  name:   dfs.namenode.http-address.组名.角色名 //指明该角色对应的namenode
  value:  主机名:50070

  name:   dfs.namenode.shared.edits.dir       //部署JNS服务
  value:  qjournal://主机1:8485;主机2:8485;主机3:8485

  name:   dfs.journalnode.edits.dir           //定义JNS服务数据的存储目录
  value:  /var/hadoop/journal

  name:   dfs.client.failover.proxy.provider.组名  //部署hadoop高可用软件
  value:  org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider

  name:   dfs.ha.fencing.methos       //指定操作其他主机的方式(ssh)
  value:  sshfence

  name:   dfs.ha.fencing.ssh.private-key-files  //指明私钥文件
  value:  /root/.ssh/id_rsa

  name:   dfs.ha.automatic-failover.enabled     //设置自动实现故障转移
  value:  true

(6)yarn-site.xml        //设置resourcemanager的高可用

#删除之前设置的resourcemanager

  name:   yarn.resourcemanager.cluster-id   //声明组名
  value:  yarn-ha

  name:   yarn.resourcemanager.ha.rm-ids
  value:  rm1,rm2                         //设置了两个角色

#有几个角色写几个键值对
  name:   yarn.resourcemanager.hostname.角色名  //指明角色对应的主机(实现高可用)
  value:  主机名

  name:   yarn.resourcemanager.ha.enabled     //开启resourcemanager的高可用
  value:  true

  name:   yarn.resourcemanager.recovery.enabled
  value:  true                            //自动实现故障转移

  name:   yarn.resourcemanager.store.class    //指明存储的类
  value:  org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore

  name:   yarn.resourcemanager.zk-address   //使用zookeeper集群
  value:  主机1:2181,主机2:2181             //多写几台

三.进行部署

1.清空所以的hadoop数据(确保有数据目录,且下面都是空的)

2.同步配置文件到所以机器

3.初始化zookeeper集群
  hadoop/bin/hafs zkfc -formatZK

4.在相应主机上启动journalnode服务     //是为了初始化
  hadoop/sbin/hadoop-daemon.sh start journalnode

5.主namenode格式化hadoop数据目录(必须先启动journalnode,因为其会接收变更的数据日志)
  hadoop/bin/hdfs namenode -format

6.主namenode将数据目录同步到从namenode(必须保持一致,不能格式化)

7.在namenode上初始化journalnode
  hadoop/bin/hdfs namenode -initializeSharedEdits

8.在相应主机上停掉journalnode服务       //为了使用hadoop统一管理
  hadoop/sbin/hadoop-daemon.sh stop journalnode

9.在主namenode上启动集群
  hadoop/sbin/start-all.sh        //或者分别执行:start-dfs.sh和start-yarn.sh

10.在从namenode上启动resourcemanager热备
  hadoop/yarn-daemon.sh start resourcemanager

四.验证
  hadoop/bin/hdfs haadmin -getServiceState 角色名
  hadoop/bin/yarn rmadmin -getServiceState 角色名
  hadoop/bin/hdfs dfsadmin -report
  hadoop/bin/yarn node -list
  hadoop/bin/hadoop fs -ls /

###############################################################################
